{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-29T22:37:39.612999Z",
     "iopub.status.busy": "2025-04-29T22:37:39.612727Z",
     "iopub.status.idle": "2025-04-30T00:09:50.013762Z",
     "shell.execute_reply": "2025-04-30T00:09:50.012717Z",
     "shell.execute_reply.started": "2025-04-29T22:37:39.612975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Install pennylane\u001b[39;00m\n\u001b[32m      2\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install pennylane --quiet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Install pennylane\n",
    "!pip install pennylane --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from collections import Counter\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import uuid\n",
    "import os\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "qml.numpy.random.seed(seed)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom transform for contrast enhancement\n",
    "class AdjustContrast:\n",
    "    def __init__(self, factor=2.0):\n",
    "        self.factor = factor\n",
    "    def __call__(self, img):\n",
    "        return transforms.functional.adjust_contrast(img, self.factor)\n",
    "\n",
    "# Define transformations with reduced augmentation\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    AdjustContrast(2.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define transformations for testing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    AdjustContrast(2.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom dataset to handle corrupted files\n",
    "class RobustImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.dataset = datasets.ImageFolder(root, transform=None)\n",
    "        self.transform = transform\n",
    "        self.valid_indices = []\n",
    "        self.skipped_files = []\n",
    "        \n",
    "        # Verify files\n",
    "        for idx in range(len(self.dataset)):\n",
    "            try:\n",
    "                path, _ = self.dataset.samples[idx]\n",
    "                with open(path, 'rb') as f:\n",
    "                    Image.open(f).verify()\n",
    "                self.valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                self.skipped_files.append((path, str(e)))\n",
    "                logger.warning(f\"Skipped file {path}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Total files: {len(self.dataset)}, Valid files: {len(self.valid_indices)}, Skipped: {len(self.skipped_files)}\")\n",
    "        if self.skipped_files:\n",
    "            for path, error in self.skipped_files[:5]:\n",
    "                logger.info(f\"Example skipped file: {path} (Error: {error})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        path, target = self.dataset.samples[actual_idx]\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, target\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {path}: {e}\")\n",
    "            raise\n",
    "\n",
    "# Dataset path\n",
    "data_dir = \"/kaggle/input/mmmmmmm/dataset_blood_group\"\n",
    "print(\"Checking dataset structure:\")\n",
    "!ls {data_dir}\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    dataset = RobustImageFolder(data_dir, transform=data_transform)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset at {data_dir}: {e}\")\n",
    "    raise\n",
    "\n",
    "class_names = dataset.dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Analyze class distribution\n",
    "class_counts = Counter(dataset.dataset.targets[i] for i in dataset.valid_indices)\n",
    "print(\"Class distribution:\", {class_names[i]: count for i, count in class_counts.items()})\n",
    "\n",
    "# Split into training and validation (80/20)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "# Compute class weights\n",
    "class_counts_array = np.array([class_counts[i] for i in range(len(class_names))])\n",
    "class_weights = torch.tensor(1.0 / class_counts_array, dtype=torch.float).to(device)\n",
    "\n",
    "# Define classical CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 8)\n",
    "        self.shortcut = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.pool(F.relu(self.bn1(self.conv1(x))))  # 224→112\n",
    "        x2 = self.pool(F.relu(self.bn2(self.conv2(x1))))  # 112→56\n",
    "        shortcut = self.shortcut(x2)  # 56→56\n",
    "        x3 = F.relu(self.bn3(self.conv3(x2)) + shortcut)  # 56→56\n",
    "        x4 = self.pool(F.relu(self.bn4(self.conv4(x3))))  # 56→28\n",
    "        x5 = x4.view(-1, 128 * 28 * 28)\n",
    "        x6 = F.relu(self.fc1(x5))\n",
    "        x7 = self.dropout(x6)\n",
    "        x8 = self.fc2(x7)\n",
    "        return x8\n",
    "\n",
    "# Define quantum circuit\n",
    "n_qubits = 4\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def quantum_circuit(inputs, weights):\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True)\n",
    "    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "# Define quantum layer\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        weight_shapes = {\"weights\": (2, n_qubits)}\n",
    "        self.q_layer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.q_layer(x)\n",
    "\n",
    "# Define hybrid CNN\n",
    "class HybridCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.shortcut = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 64)\n",
    "        self.fc_quantum = nn.Linear(64, 16)  # For AmplitudeEmbedding\n",
    "        self.bn5 = nn.BatchNorm1d(16)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.q_layer = QuantumLayer()\n",
    "        self.fc3 = nn.Linear(4, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)) + shortcut)\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc_quantum(x))\n",
    "        x = self.bn5(x)\n",
    "        x = self.q_layer(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Function to train and evaluate\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, model_name, patience=20):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                print(\"Training...\")\n",
    "            else:\n",
    "                model.eval()\n",
    "                print(\"Validating...\")\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            try:\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                            optimizer.step()\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "                print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "                if phase == 'train':\n",
    "                    train_losses.append(epoch_loss)\n",
    "                    train_accs.append(epoch_acc.item())\n",
    "                else:\n",
    "                    val_losses.append(epoch_loss)\n",
    "                    val_accs.append(epoch_acc.item())\n",
    "                    if epoch_loss < best_val_loss:\n",
    "                        best_val_loss = epoch_loss\n",
    "                        epochs_no_improve = 0\n",
    "                        torch.save(model.state_dict(), f'/kaggle/working/best_{model_name}.pkl')\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                    if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        scheduler.step(epoch_loss)\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "                    if epochs_no_improve >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in {phase} phase: {e}\")\n",
    "                raise\n",
    "        if epochs_no_improve >= patience:\n",
    "            break\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), f'/kaggle/working/{model_name}.pkl')\n",
    "    print(f'Final model saved as {model_name}.pkl')\n",
    "    \n",
    "    # Plot training metrics\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'/kaggle/working/{model_name}_metrics.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Evaluate model with misprediction logging\n",
    "def evaluate_model(model, dataloader, model_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    mispredictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for i in range(len(labels)):\n",
    "                if preds[i] != labels[i]:\n",
    "                    mispredictions.append({\n",
    "                        'True': class_names[labels[i]],\n",
    "                        'Predicted': class_names[preds[i]],\n",
    "                        'Probs': {class_names[j]: probs[i][j].item() for j in range(8)}\n",
    "                    })\n",
    "                all_preds.append(preds[i].cpu().numpy())\n",
    "                all_labels.append(labels[i].cpu().numpy())\n",
    "    \n",
    "    # Print up to 5 mispredictions\n",
    "    print(f\"\\n{model_name} Mispredictions (up to 5):\")\n",
    "    for i, mis in enumerate(mispredictions[:5]):\n",
    "        print(f\"Misprediction {i+1}: True={mis['True']}, Predicted={mis['Predicted']}\")\n",
    "        print(f\"Probabilities: {', '.join([f'{k}: {v:.4f}' for k, v in mis['Probs'].items()])}\")\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.savefig(f'/kaggle/working/{model_name}_cm.png')\n",
    "    plt.close()\n",
    "    \n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    return cm, report\n",
    "\n",
    "# Function to test a single image\n",
    "def test_single_image(image_path, model, model_name, transform, class_names):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            predicted_class = class_names[pred.item()]\n",
    "            prob_list = {class_names[i]: probs[0][i].item() for i in range(len(class_names))}\n",
    "        print(f\"\\n{model_name} Prediction for {image_path}:\")\n",
    "        print(f\"Predicted Class: {predicted_class}\")\n",
    "        print(f\"Probabilities: {', '.join([f'{k}: {v:.4f}' for k, v in prob_list.items()])}\")\n",
    "        return predicted_class, prob_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Debug quantum layer outputs\n",
    "def debug_quantum_output(model, image_path, transform):\n",
    "    try:\n",
    "        model.eval()\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            x = model.pool(F.relu(model.bn1(model.conv1(x))))\n",
    "            x = model.pool(F.relu(model.bn2(model.conv2(x))))\n",
    "            shortcut = model.shortcut(x)\n",
    "            x = F.relu(model.bn3(model.conv3(x)) + shortcut)\n",
    "            x = model.pool(F.relu(model.bn4(model.conv4(x))))\n",
    "            x = model.flatten(x)\n",
    "            x = F.relu(model.fc1(x))\n",
    "            x = model.dropout(x)\n",
    "            x = F.relu(model.fc2(x))\n",
    "            x = F.relu(model.fc_quantum(x))\n",
    "            x = model.bn5(x)\n",
    "            q_out = model.q_layer(x)\n",
    "            print(f\"Quantum layer output for {image_path}: {q_out.cpu().numpy()}\")\n",
    "        return q_out\n",
    "    except Exception as e:\n",
    "        print(f\"Error debugging quantum output for {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Visualize test image\n",
    "def visualize_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Test Image\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig('/kaggle/working/test_image.png')\n",
    "        plt.close()\n",
    "        print(\"Test image visualized and saved as test_image.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing image {image_path}: {e}\")\n",
    "\n",
    "# Train classical CNN\n",
    "model_classical = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer_classical = optim.SGD(model_classical.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler_classical = optim.lr_scheduler.ReduceLROnPlateau(optimizer_classical, mode='min', factor=0.5, patience=5)\n",
    "train_losses_c, val_losses_c, train_accs_c, val_accs_c = train_model(\n",
    "    model_classical, criterion, optimizer_classical, scheduler_classical, \n",
    "    num_epochs=50, model_name='simple_cnn_blood_group'\n",
    ")\n",
    "print(\"\\nEvaluating SimpleCNN on validation set:\")\n",
    "cm_classical, report_classical = evaluate_model(model_classical, val_loader, \"SimpleCNN\")\n",
    "\n",
    "# Train hybrid CNN\n",
    "model_hybrid = HybridCNN().to(device)\n",
    "optimizer_hybrid = optim.AdamW([\n",
    "    {'params': model_hybrid.q_layer.parameters(), 'lr': 0.01},\n",
    "    {'params': [p for n, p in model_hybrid.named_parameters() if 'q_layer' not in n], 'lr': 0.001}\n",
    "], weight_decay=1e-4)\n",
    "scheduler_hybrid = optim.lr_scheduler.ReduceLROnPlateau(optimizer_hybrid, mode='min', factor=0.5, patience=5)\n",
    "train_losses_h, val_losses_h, train_accs_h, val_accs_h = train_model(\n",
    "    model_hybrid, criterion, optimizer_hybrid, scheduler_hybrid, \n",
    "    num_epochs=50, model_name='hybrid_cnn_blood_group'\n",
    ")\n",
    "print(\"\\nEvaluating HybridCNN on validation set:\")\n",
    "cm_hybrid, report_hybrid = evaluate_model(model_hybrid, val_loader, \"HybridCNN\")\n",
    "\n",
    "# Compare models\n",
    "results = {\n",
    "    'Model': ['SimpleCNN', 'HybridCNN'],\n",
    "    'Val Accuracy': [max(val_accs_c), max(val_accs_h)],\n",
    "    'Val Loss': [min(val_losses_c), min(val_losses_h)],\n",
    "    'Macro F1': [report_classical['macro avg']['f1-score'], report_hybrid['macro avg']['f1-score']]\n",
    "}\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(pd.DataFrame(results))\n",
    "\n",
    "# Save comparison\n",
    "pd.DataFrame(results).to_csv('/kaggle/working/model_comparison.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T00:31:55.602187Z",
     "iopub.status.busy": "2025-04-30T00:31:55.601896Z",
     "iopub.status.idle": "2025-04-30T00:31:56.377378Z",
     "shell.execute_reply": "2025-04-30T00:31:56.376732Z",
     "shell.execute_reply.started": "2025-04-30T00:31:55.602166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing image: /kaggle/input/mmmmmmm/dataset_blood_group/B-/cluster_3_1018.BMP\n",
      "Test image visualized and saved as test_image.png\n",
      "Error debugging quantum output for /kaggle/input/mmmmmmm/dataset_blood_group/B-/cluster_3_1018.BMP: cannot access local variable 'x' where it is not associated with a value\n",
      "\n",
      "SimpleCNN Prediction for /kaggle/input/mmmmmmm/dataset_blood_group/B-/cluster_3_1018.BMP:\n",
      "Predicted Class: B-\n",
      "Probabilities: A+: 0.0000, A-: 0.0001, AB+: 0.0000, AB-: 0.0000, B+: 0.0000, B-: 0.9999, O+: 0.0000, O-: 0.0000\n",
      "\n",
      "HybridCNN Prediction for /kaggle/input/mmmmmmm/dataset_blood_group/B-/cluster_3_1018.BMP:\n",
      "Predicted Class: B-\n",
      "Probabilities: A+: 0.0001, A-: 0.1617, AB+: 0.0007, AB-: 0.0070, B+: 0.0104, B-: 0.8169, O+: 0.0032, O-: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('B-',\n",
       " {'A+': 8.974920638138428e-05,\n",
       "  'A-': 0.16171498596668243,\n",
       "  'AB+': 0.0007081134244799614,\n",
       "  'AB-': 0.006993262097239494,\n",
       "  'B+': 0.01041414961218834,\n",
       "  'B-': 0.8169117569923401,\n",
       "  'O+': 0.003157068509608507,\n",
       "  'O-': 1.0802724318637047e-05})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test single image\n",
    "image_path = \"/kaggle/input/mmmmmmm/dataset_blood_group/B-/cluster_3_1018.BMP\"\n",
    "print(f\"\\nTesting image: {image_path}\")\n",
    "\n",
    "# Visualize test image\n",
    "visualize_image(image_path)\n",
    "\n",
    "# Debug quantum layer for HybridCNN\n",
    "debug_quantum_output(model_hybrid, image_path, test_transform)\n",
    "\n",
    "# Load and test with SimpleCNN\n",
    "try:\n",
    "    model_classical.load_state_dict(torch.load('/kaggle/working/simple_cnn_blood_group.pkl'))\n",
    "except FileNotFoundError:\n",
    "    print(\"SimpleCNN model file not found. Using the last trained model.\")\n",
    "test_single_image(image_path, model_classical, \"SimpleCNN\", test_transform, class_names)\n",
    "\n",
    "# Load and test with HybridCNN\n",
    "try:\n",
    "    model_hybrid.load_state_dict(torch.load('/kaggle/working/hybrid_cnn_blood_group.pkl'))\n",
    "except FileNotFoundError:\n",
    "    print(\"HybridCNN model file not found. Using the last trained model.\")\n",
    "test_single_image(image_path, model_hybrid, \"HybridCNN\", test_transform, class_names)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7215887,
     "sourceId": 11508302,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7289113,
     "sourceId": 11619190,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
